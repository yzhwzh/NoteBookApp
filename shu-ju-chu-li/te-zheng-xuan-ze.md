---
description: Feature Selection
---

# 特征选择

### 介绍

1. 对于一个学习任务，给定了属性集，其中某些属性可能对于学习来说是很关键的，但是有些属性可能就意义不大。

   * 对当前学习任务有用的属性称作相关特征`relevant feature` 。
   * 对当前学习任务没有用的属性称作无关特征`irrelevant feature` 。

   从给定的特征集合中选出相关特征子集的过程称作特征选择`feature selection`。

2. 特征选择可能会降低模型的预测能力。因为被剔除的特征中可能包含了有效的信息，抛弃了这部分信息会一定程度上降低预测准确率。

   这是计算复杂度和预测能力之间的折衷：

   * 如果保留尽可能多的特征，则模型的预测能力会有所提升，但是计算复杂度会上升。
   * 如果剔除尽可能多的特征，则模型的预测能力会有所下降，但是计算复杂度会下降。

### 特征选择原理

1. 特征选择是一个重要的数据预处理（`data preprocessing`）过程。在现实机器学习任务中，获取数据之后通常首先进行特征选择，然后再训练学习器。

   进行特征选择的原因：  


   * 首先，在现实任务中经常会遇到维数灾难问题，这是由于属性过多造成的。如果能从中选择出重要的特征，使得后续学习过程仅仅需要在一部分特征上构建模型，则维数灾难问题会大大减轻。

     从这个意义上讲，特征选择与降维技术有相似的动机。事实上它们是处理高维数据的两大主流技术。

   * 其次，去除不相关特征往往会降低学习任务的难度。

2. 特征选择过程必须确保不丢失重要特征，否则后续学习过程会因为重要信息的缺失而无法获得很好的性能。
   * 给定数据集，如果学习任务不同，则相关特征很可能不同，因此特征选择中的无关特征指的是与当前学习任务无关的特征。
   * 有一类特征称作冗余特征`redundant feature`，它们所包含的信息能从其他特征中推演出来。

     * 冗余特征在很多时候不起作用，去除它们能够减轻学习过程的负担。
     * 但如果冗余特征恰好对应了完成学习任务所需要的某个中间概念，则该冗余特征是有益的，能降低学习任务的难度。

     这里暂且不讨论冗余特征，且假设初始的特征集合包含了所有的重要信息。
3. 要想从初始的特征集合中选取一个包含了所有重要信息的特征子集，如果没有任何领域知识作为先验假设，则只能遍历所有可能的特征组合。

   这在计算上是不可行的，因为这样会遭遇组合爆炸，特征数量稍多就无法进行。

   一个可选的方案是：

   * 产生一个候选子集，评价出它的好坏。
   * 基于评价结果产生下一个候选子集，再评价其好坏。
   * 这个过程持续进行下去，直至无法找到更好的后续子集为止。

   这里有两个问题：如何根据评价结果获取下一个候选特征子集？如何评价候选特征子集的好坏？

#### **子集评价**

1. 如何评价候选特征子集的好坏？这是一个子集评价`subset evaluation`问题。
2. 给定数据集$$D$$ ，假设所有属性均为离散型。对属性子集$$A$$  ， 假定根据其**取值**将$$D$$分成了$$V$$个子集： $$\{D_1,D_2,...,D_v\}$$ 

   于是可以计算属性子集$$A$$ 的信息增益：$$g(D,A) = H(D) - H(D|A) = H(D) - \sum_{v=1}^V \frac{|D_v|}{|D|}H(D_v)$$

   其中$$|.|$$  为集合大小，$$H(.)$$  为熵。

   信息增益越大，则表明特征子集 $$A$$包含的有助于分类的信息越多。于是对于每个候选特征子集，可以基于训练数据集 $$D$$ 来计算其信息增益作为评价准则。

3. 更一般地，特征子集$$A$$ 实际上确定了对数据集$$D$$  的一个划分规则。
   * 每个划分区域对应着$$A$$上的一个取值，而样本标记信息$$y$$  则对应着$$D$$  的真实划分。
   * 通过估算这两种划分之间的差异，就能对$$A$$  进行评价：与$$y$$  对应的划分的差异越小，则说明  越好。
   * 信息熵仅仅是判断这个差异的一种方法，其他能判断这两个划分差异的机制都能够用于特征子集的评价。
4. 将特征子集搜索机制与子集评价机制结合就能得到特征选择方法。
   * 事实上，**决策树可以用于特征选择**，所有树结点的划分属性所组成的集合就是选择出来的特征子集。
   * 其他特征选择方法本质上都是显式或者隐式地结合了某些子集搜索机制和子集评价机制。
5. 常见的特征选择方法大致可分为三类：过滤式`filter`、包裹式`wrapper`、嵌入式`embedding` 。

#### 过滤式选择

1. 过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关。

   这相当于先用特征选择过程对初始特征进行过滤，再用过滤后的特征来训练模型。

2. `Relief:Relevant Features`是一种著名的过滤式特征选择方法，该方法设计了一个相关统计量来度量特征的重要性。
   * 该统计量是一个向量，其中每个分量都对应于一个初始特征。特征子集的重要性则是由该子集中每个特征所对应的相关统计量分量之和来决定的。
   * 最终只需要指定一个阈值$$\tau$$，然后选择比$$\tau$$大的相关统计量分量所对应的特征即可。

     也可以指定特征个数 $$k$$ ，然后选择相关统计量分量最大的 $$k$$个特征。
3. 给定训练集 $$\mathbb D=\{(\mathbf{\vec x}_1,\tilde y_1),(\mathbf{\vec x}_2,\tilde y_2),\cdots,(\mathbf{\vec x}_N,\tilde y_N)\},\tilde y_i\in \{0,1\}$$  。 对于每个样本 $$\mathbf{\vec x}_i$$  :  


   * `Relief` 先在 $$\mathbf{\vec x}_i$$ 同类样本中寻找其最近邻 $$\mathbf{\vec x}_{nh_i}$$ ，称作猜中近邻`near-hit` 。
   * 然后从 $$\vec x_i$$ 的异类样本中寻找其最近邻 $$\mathbf{\vec x}_{nm_i}$$，称作猜错近邻`near-miss` 。
   * 然后相关统计量对应于属性$$j$$ 的分量为： $$\delta_j=\sum_{i=1}^{N}\left(-\text{diff}(x_{i,j},x_{nh_i,j})^2+\text{diff}(x_{i,j},x_{nm_i,j})^{2}\right)$$ 

   其中 $$\text{diff}(x_{a,j},x_{b,j})$$  为两个样本在属性 $$j$$ 上的差异值，其结果取决于该属性是离散的还是连续的：

   * 如果属性$$j$$ 是离散的，则： $$\text{diff}(x_{a,j},x_{b,j})=\begin{cases} 0,&\text{if}\quad  x_{a,j}=x_{b,j}\\ 1,&else \end{cases}$$ 
   * 如果属性 $$j$$ 是连续的，则： $$\text{diff}(x_{a,j},x_{b,j})=| x_{a,j}-x_{b,j}|$$ 

     注意：此时 $$ x_{a,j},x_{b,j}$$  需要标准化到 $$[0,1]$$ 区间。

4. 从公式 $$\delta_j=\sum_{i=1}^{N}\left(-\text{diff}(x_{i,j},x_{nh_i,j})^2+\text{diff}(x_{i,j},x_{nm_i,j})^{2}\right)$$ 

   可以看出：

   * 如果 $$\mathbf{\vec x}_i$$ 与其猜中近邻 $$\mathbf{\vec x}_{nh_i}$$ 在属性 $$j$$ 上的距离小于 $$\mathbf{\vec x}_i$$ 与其猜错近邻 $$\mathbf{\vec x}_{nm_i}$$ 的距离，则说明属性 $$j$$  对于区分同类与异类样本是有益的，于是增大属性 $$j$$ 所对应的统计量分量。
   * 如果 $$\mathbf{\vec x}_i$$  与其猜中近邻 $$\mathbf{\vec x}_{nh_i}$$ 在属性 $$j$$ 上的距离大于 $$\mathbf{\vec x}_i$$ 与其猜错近邻 $$\mathbf{\vec x}_{nm_i}$$ 的距离，则说明属性$$j $$对于区分同类与异类样本是起负作用的，于是减小属性$$j$$ 所对应的统计量分量。
   * 最后对基于不同样本得到的估计结果进行平均，就得到各属性的相关统计量分量。分量值越大，则对应属性的分类能力越强。

5. `Relief` 是为二分类问题设计的，其扩展变体 `Relief-F` 能处理多分类问题。

   假定数据集$$D$$ 中的样本类别为： $$c_1,c_2,\cdots,c_K$$  。对于样本 $$\mathbf{\vec x}_i$$ ，假设 $$\tilde y_i=c_k$$ 。

   * `Relief-F` 先在类别 $$c_k$$ 的样本中寻找 $$\mathbf{\vec x}_i$$  的最近邻 $$\mathbf{\vec x}_{nh_i}$$  作为猜中近邻。
   * 然后在 $$c_k$$ 之外的每个类别中分别找到一个 $$\mathbf{\vec x}_i$$ 的最近邻 $$\mathbf{\vec x}_{nm_i^l} ,l=1,2,\cdots,K;l\ne k$$ 作为猜错近邻。
   * 于是相关统计量对应于属性 $$j $$  的分量为： $$\delta_j=\sum_{i=1}^{N}\left(-\text{diff}(x_{i,j},x_{nh_i,j})^{2}+\sum_{l\ne k}\left(p_l\times\text{diff}(x_ {i,j},x_{nm_i^l,j})^{2}\right)\right)$$ 

     其中 $$p_l$$  为第$$l$$  类的样本在数据集 $$D$$ 中所占的比例。

