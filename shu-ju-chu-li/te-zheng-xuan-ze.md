---
description: Feature Selection
---

# 特征选择

### 介绍

1. 对于一个学习任务，给定了属性集，其中某些属性可能对于学习来说是很关键的，但是有些属性可能就意义不大。

   * 对当前学习任务有用的属性称作相关特征`relevant feature` 。
   * 对当前学习任务没有用的属性称作无关特征`irrelevant feature` 。

   从给定的特征集合中选出相关特征子集的过程称作特征选择`feature selection`。

2. 特征选择可能会降低模型的预测能力。因为被剔除的特征中可能包含了有效的信息，抛弃了这部分信息会一定程度上降低预测准确率。

   这是计算复杂度和预测能力之间的折衷：

   * 如果保留尽可能多的特征，则模型的预测能力会有所提升，但是计算复杂度会上升。
   * 如果剔除尽可能多的特征，则模型的预测能力会有所下降，但是计算复杂度会下降。

### 特征选择原理

1. 特征选择是一个重要的数据预处理（`data preprocessing`）过程。在现实机器学习任务中，获取数据之后通常首先进行特征选择，然后再训练学习器。

   进行特征选择的原因：  


   * 首先，在现实任务中经常会遇到维数灾难问题，这是由于属性过多造成的。如果能从中选择出重要的特征，使得后续学习过程仅仅需要在一部分特征上构建模型，则维数灾难问题会大大减轻。

     从这个意义上讲，特征选择与降维技术有相似的动机。事实上它们是处理高维数据的两大主流技术。

   * 其次，去除不相关特征往往会降低学习任务的难度。

2. 特征选择过程必须确保不丢失重要特征，否则后续学习过程会因为重要信息的缺失而无法获得很好的性能。
   * 给定数据集，如果学习任务不同，则相关特征很可能不同，因此特征选择中的无关特征指的是与当前学习任务无关的特征。
   * 有一类特征称作冗余特征`redundant feature`，它们所包含的信息能从其他特征中推演出来。

     * 冗余特征在很多时候不起作用，去除它们能够减轻学习过程的负担。
     * 但如果冗余特征恰好对应了完成学习任务所需要的某个中间概念，则该冗余特征是有益的，能降低学习任务的难度。

     这里暂且不讨论冗余特征，且假设初始的特征集合包含了所有的重要信息。
3. 要想从初始的特征集合中选取一个包含了所有重要信息的特征子集，如果没有任何领域知识作为先验假设，则只能遍历所有可能的特征组合。

   这在计算上是不可行的，因为这样会遭遇组合爆炸，特征数量稍多就无法进行。

   一个可选的方案是：

   * 产生一个候选子集，评价出它的好坏。
   * 基于评价结果产生下一个候选子集，再评价其好坏。
   * 这个过程持续进行下去，直至无法找到更好的后续子集为止。

   这里有两个问题：如何根据评价结果获取下一个候选特征子集？如何评价候选特征子集的好坏？

#### **子集评价**

1. 如何评价候选特征子集的好坏？这是一个子集评价`subset evaluation`问题。

