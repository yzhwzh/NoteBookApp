---
description: Principal Component Analysis   &  Singular Value Decomposition
---

# 主成分分析与奇异值分解

### 前言：

        矩阵降维与矩阵分解是两个不同的概念。矩阵降维指就是把数据从高维空间投影到一个低维空间，这个过程可以通过线性或者非线性的映射来完成。目的是挖掘出高维数据中富含原始信息的低维嵌入表示。降维显然是有代价的，它造成了原始信息的损失。所以降维算法的重点和难点在于如何在对原始数据进行数据降维的过程中还能尽可能地保持高维数据的几何结构信息或本征的有区别性的信息，并在此前提下找到高维数据的最优低维表示。对于传统的降维算法来说，它们通常的考虑角度都是找到最大可能地保持某种信息的投影方向或者低维空间。而矩阵分解是将矩阵拆解为数个矩阵的乘积，并没有改变矩阵的维数，但通过矩阵分解往往可以从复杂的数据中提取出相对重要的特征信息，如在特征向量为基的各个方向上的投影，然后通过保留较大的投影，删除较小的投影，来实现数据压缩和降维的目的，例如特征分解：

$$
M=A\begin{bmatrix}{\lambda_1}&{0}&{\cdots}&{0}\\{0}&{\lambda2}&{\cdots}&{0}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{0}&{0}&{\cdots}&{\lambda_n}\end{bmatrix}A^{-1}=A\begin{bmatrix}{\lambda_1}&{0}&{\cdots}&{0}\\{0}&{0}&{\cdots}&{0}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{0}&{0}&{\cdots}&{0}\end{bmatrix}A^{-1}+A\begin{bmatrix}{0}&{0}&{\cdots}&{0}\\{0}&{\lambda2}&{\cdots}&{0}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{0}&{0}&{\cdots}&{0}\end{bmatrix}A^{-1}+...+A\begin{bmatrix}{0}&{0}&{\cdots}&{0}\\{0}&{0}&{\cdots}&{0}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{0}&{0}&{\cdots}&{\lambda_n}\end{bmatrix}A^{-1}
$$

，进而可写成秩逼近的形式：

$$
M=\lambda_1[V_1,0,...,0]A^{-1}+\lambda_2[0,V_2,...,0]A^{-1}+...+\lambda_n[0,0,...,V_n]A^{-1} = \lambda_1V_1\mu_1^T + \lambda_2V_2\mu_2^T+...+\lambda_nV_n\mu_n^T
$$

其中 $$V_i$$ 是特征向量，列向量； $$\mu_i^T$$ 为逆矩阵对应的行向量，若是实对称阵 $$\mu_i^T = V_i^T$$ 。故可通过删除小特征值对应方向的数据，只保留$$K$$个大特征值方向对应的数据来进行压缩。特征值越大，说明矩阵在对应的特征向量上的方差越大，功率越大，信息量越多，同时也可将矩阵看做是由  $$n*K,K*K,K*n$$ 三个小矩阵来近似描述，从而也达到了降维的目的。因而矩阵分解是矩阵降维的一种重要方法。比较常用的矩阵分解方法有EVD和SVD，降维思想则有PCA, LDA, NMF, MDS等，下面介绍一下相关的学习笔记。

### PCA \(Principal Component Analysis\):

        PCA即主成分分析，即设法将原来$$n$$个有一定相关的指标，重新组合成一组新的线性无关的综合指标来代替原来指标。也就是将$$n$$维特征映射到$$k$$维上，这$$k$$维是全新的正交特征也被称为主成分，是在原有$$n$$维特征的基础上重新构造出来的$$k$$维特征。

> **理解矩阵乘法**
>
> 内积与投影  
>  $$A*B=|A||B|cos\theta$$  含义为$$ A$$与$$B$$的内积等于$$A$$在$$B$$上的投影长度乘以$$B $$的模，若设向量$$B$$的模为1，则$$A$$与$$B$$的内积值等于$$A$$向$$B$$所在直线投影的矢量长度，即 $$A*B=|A|cos\theta$$ 
>
> 基向量  
> 要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了。 例如向量 $$(x,y)$$ 实际上表示线性组合： $$\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix} = x(1,0)^T+y(0,1)^T$$ 
>
> 基变换的矩阵表示  
> 如果我们有$$M$$个$$N$$维向量，想将其变换为由$$R$$个$$N$$维向量表示的新空间中，那么首先将$$R$$个基**按行**组成矩阵$$A$$，然后将向量按列组成矩阵$$B$$，那么两矩阵的乘积$$AB$$就是变换结果，其中$$AB$$的第$$m$$列为$$A$$中第$$m$$列变换后的结果。
>
> **两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。**

        因而PCA的核心，既是要寻找一组基组成的矩阵左乘原矩阵，使得$$n$$维特征映射到$$k$$维上。那么我们应该如何选择$$k$$个基才能最大程度保留原有的信息？

        一种直观的看法是：希望投影后的投影值尽可能分散。而这种分散程度，可以用数学上的方差来表述，即投影后的数值的方差 $$Var(a) = \frac{1}{m}\sum_{i=1}^m(a_i-u)^2$$ ; 如对数据已经均值化了，则 $$Var(a) = \frac{1}{m}\sum_{i=1}^ma_i^2$$ 。方差越大，表明保留的信息越多。

        在高纬度中，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此应该有其他约束条件。从直观上说，让投影尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。协方差表示其相关性，由于已经让每个变量均值为0，则： $$Cov(a,b)=\frac{1}{m}\sum_{i=1}^ma_ib_i$$ ，为了让协方差为0，选择第二个基时只能在与第一个基正交的方向上选择。以此类推，第三个基与第一和第二存在的平面正交。将一组$$N$$维向量降为$$K$$维（$$K$$大于0，小于$$N$$），其目标是选择$$K$$个单位（模为1）正交基，使得原始数据变换到这组基上后，各变量两两间协方差为0，而变量的方差则尽可能大（在正交的约束下，取最大的$$K$$个方差）。

#### **PCA 本质**：

        设原始数据矩阵$$X$$，而$$P$$是一组基按行组成的矩阵，设$$Y=PX$$，则$$Y$$为$$X$$对P做基变换后的数据（因为$$X$$每行的的均值为0，左乘矩阵相当于对$$X$$每行乘以一个系数，故变换后，每行均值仍为0）。此时，$$Y$$的协方差矩阵为 $$D=\frac{1}{m}YY^T$$ ，根据PCA选择基向量的约束条件，则要求对角线上的方差大，而不同基向量的协方差为0，则$$D$$为对角矩阵；进行代换 $$\frac{1}{m}YY^T = \frac{1}{m}PX(PX)^T = \frac{1}{m}PXX^TP^T$$ ,因而可以看出寻找的基向量矩阵，即是为能够让原始矩阵$$X$$的协方差阵对角化的矩阵，换句话说，优化目标变成了寻找一个矩阵$$P$$，满足 $$PCP^T$$ 是一个对角阵，并且对角元素按从大到小依次排列，那么$$P$$的前$$K$$行就是要寻找的基，用$$P$$的前$$K$$行组成的矩阵乘以$$X$$就使得$$X$$从$$N$$维降到了$$K$$维并满足上述优化条件,\(其中$$P$$为$$C$$的特征向量组成矩阵的逆矩阵，因为$$C$$为对称阵，故$$P$$即特征矩阵的转置，前$$K$$行即为前$$K$$个特征向量\)。

### **SVD \(Singular Value Decomposition\)**：

        SVD即奇异值分解，是特征分解在任意矩阵上的推广。因为特征分解，是有约束条件的，矩阵为方阵，并需存在可逆矩阵$$A$$，使得 $$A^{-1}MA$$ 为对角阵，同时还不能保证特征值一定是实数。因而在此基础上对 $$MM^T$$ 进行分解，**因为** $$MM^T$$ **是实对称阵，必有一个正交矩阵使之对角化**，不同特征值对应的特征向量一定正交，且特征值都是实数，特征向量都是实向量，因而能在任意矩阵上进行应用。

        推导过程：对于任意矩阵, $$A^TA$$ 为$$n$$阶对称矩阵，可以将之作特征分解: $$A^TA=VDV^T$$ ;这个时候我们得到一组正交基 $$\{v_1,v_2,...,v_n\}$$ ，其特征值为 $$\lambda_i$$ ，即 $$A^TAv_i=\lambda_i v_i$$ ; 此时可以推导出 $$\{Av_1,Av_2,...,Av_n\}$$ 也是一组正交基，即 

$$
(Av_i)^T(Av_j)=v_i^TA^TAv_j=v_i^T(\lambda_jv_j)=\lambda_jv_i^Tv_j=0
$$

，只是没有标准化，进一步标准化，令

$$
u_i=\frac{Av_i}{|Av_i|}; |Av_i| = \sqrt{(Av_i)^T(Av_i)} = \sqrt{\lambda_i}
$$

  故 $$Av_i=\sqrt{\lambda_i}u_i=\sigma_i u_i$$ ，将向量组 $${u_1,u_2,... ,u_r}$$ 扩充为 $$R^m$$ 中的标准正交基 $${u_1,u_2,...,u_r,...,u_m}$$ 。则：

$$
AV=A\begin{bmatrix} v_1 & v_2 & ... & v_n \end{bmatrix}= \begin{bmatrix} v_1 & v_2 & ... & v_r & 0 & ... & 0 \end{bmatrix} = \begin{bmatrix} \sigma_1u_1 & \sigma_2 u_2 & ... & \sigma_r u_r & 0 &...&0 \end{bmatrix}= U\Sigma
$$

进而推出 $$A=U\Sigma V^T$$ 这就表明任意的矩阵$$A$$是可以分解成三个矩阵，$$V$$**表明了原始域的标准正交基，**$$U$$**表示经过**$$A$$**变换后的标准正交基,** $$\Sigma$$ **表示了**$$V$$**中的向量与**$$U$$**中相对应向量之间的关系**

#### **SVD几何理解**

矩阵相乘可以认为是一种线性变换，而且这种线性变换的作用效果与基的选择有关

以$$Ax = b$$为例，$$x$$是$$m$$维向量，$$b$$是$$n$$维向量，$$m,n$$可以相等也可以不相等，表示矩阵可以将一个向量线性变换到另一个向量，这样一个线性变换的作用可以包含**旋转、缩放和投影三种类型的效应**。

奇异值分解正是对线性变换这三种效应的一个析构。

$$A=U\Sigma V^T$$ 它表示我们找到了$$U$$和$$V$$这样两组基，$$A$$矩阵的作用是将一个向量从$$V$$这组正交基向量的空间旋转到$$U$$这组正交基向量空间，并对每个方向进行了一定的缩放，缩放因子就是各个奇异值。如果$$V$$维度比$$U$$大，则表示还进行了投影。可以说奇异值分解将一个矩阵原本混合在一起的三种作用效果，分解出来了。

而特征值分解其实是对旋转缩放两种效应的归并。**有投影效应的矩阵不是方阵，没有特征值**。特征值，特征向量由$$Ax=\lambda x$$得到，它表示如果一个向量$$v$$处于$$A$$的特征向量方向，那么$$Av$$对$$v$$的线性变换作用只是一个缩放。也就是说，求特征向量和特征值的过程，我们找到了这样一组基，在这组基下，矩阵的作用效果仅仅是纯存粹的缩放。**对于实对称矩阵，特征向量正交，我们可以将特征向量式子写成**$$A=X\lambda X^{T}$$**, 这样就和奇异值分解类似了， 就是**$$A$$**矩阵将一个向量从**$$X$$**这组基的空间旋转到**$$X$$**这组基的空间，并在每个方向进行了缩放，由于前后都是**$$X$$**，就是没有旋转或者理解为旋转了0度**。正交基特征向量，这样变换后才能保证变换最大的方向在基方向。**如果特征向量不正交就有可能不是变化最大的方向**

总结一下，特征值分解和奇异值分解都是给一个矩阵\(线性变换\)找一组特殊的基，特征值分解找到了特征向量这组基，在这组基下该线性变换只有缩放效果。而奇异值分解则是找到另一组基，这组基下线性变换的旋转、缩放、投影三种功能独立地展示出来了。我感觉特征值分解其实是一种找特殊角度，让旋转效果不显露出来，所以并不是所有矩阵都能找到这样巧妙的角度。

\*\*\*\*

#### 求解

        在大部分介绍中，一般会这样求解，若 $$AA^T=P\Lambda_1P^T$$ , $$A^TA=Q\Lambda_2Q^T$$ ,则矩阵A的奇异值分解为 $$A=P\Sigma Q^T$$ 。其中$$P$$为左奇异向量矩阵，$$Q$$为右奇异向量矩阵，奇异值则是 $$\sqrt{\lambda_i}$$ （ $$A^TA$$ 的特征值一定非负）; $$AA^T$$ 与 $$A^TA$$ 的对角阵虽然大小不一样，但特征值却是一样的，只是高纬度的可能有重根。

> 证明：对于任意 $$A_{m*n}$$ 和 $$B_{n*m}$$ ,$$AB$$的非0特征值与$$BA$$是一样的。 if $$ABv=\lambda v$$,$$\lambda \neq 0,v \neq 0 $$ 推出$$Bv \neq 0$$,则$$BA(Bv) = B(ABv) = \lambda Bv$$

        这样求解大部分情况是合适的，但当$$A$$ 为对称阵，若有负特征值，会产生误解，因为 $$AA^T=A^TA=A^2$$ ,其特征向量与$$A$$相同，但又要保证奇异值为负，所以会产生一点bug。而通过 $$u_i=\frac{Av_i}{|Av_i|}$$ ，则将负号转移到左奇异矩阵中了。

        同样的，通过SVD，矩阵也能写成秩逼近的形式， $$A = P\Sigma Q^T = \sum_{i=1}^k \sigma_i \vec p_i \vec q_i^T$$ 其中，任意一个 $$\sigma_i \vec p_i \vec q_i^T$$ 是原矩阵的一个秩逼近，所带来的误差是**剩余奇异值的平方和**。进而可以通过保留前$$k$$个秩逼近，来对矩阵进行降维和压缩。如给一个很大的矩阵 $$A_{m*n}$$ 需要储存 $$m*n$$ 个元素，但如果用$$k$$个秩和逼近,则只需存储$$k(m+n+1)$$个元素，即$$k$$个奇异值，$$km$$个左奇异向量元素,$$kn$$个右奇异向量元素。

> 例：矩阵 $$A = \begin{bmatrix} 1 & 2 \\ 0 & 0 \\ 0 & 0 \end{bmatrix}$$, 由$$AA^T = \begin{bmatrix} 5 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$$得到特征值$$\lambda_1 = 5, \lambda_2 = \lambda_3 = 0$$,特征向量 $$\vec p_1 = (1,0,0)^T, \vec p_2 = (0,1,0)^T, \vec p_3 = (0,0,1)^T$$;由$$A^TA = \begin{bmatrix} 1 & 2 \\ 2 & 4\end{bmatrix}$$得到特征值$$\lambda_1 = 5, \lambda_2 = 0$$,特征向量$$\vec q_1 = (\sqrt{5}/5,2\sqrt{5}/5)^T,\vec q_2 = (-2\sqrt{5}/5,\sqrt{5}/5,)^T$$; 故
>
> $$A=\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}\begin{bmatrix} \sqrt{5} & 0 \\ 0 & 0 \\ 0 & 0 \end{bmatrix}\begin{bmatrix} \frac{\sqrt{5}}{5} & \frac{2\sqrt{5}}{5} \\ -\frac{2\sqrt{5}}{5}& \frac{\sqrt{5}}{5} \end{bmatrix} = \sqrt{5}*\begin{bmatrix} 1 \\ 0 \\ 0\end{bmatrix} * [\frac{\sqrt{5}}{5},\frac{2\sqrt{5}}{5}] = \begin{bmatrix} 1 & 2 \\ 0 & 0 \\ 0 & 0 \end{bmatrix}$$

拓展延伸  
向量$$\vec p_i$$之间相互正交，$$\vec q_i$$之间也相互正交，故内积$$<\sigma_i \vec p_i \vec q_i^T,\sigma_j \vec p_j \vec q_j^T> = 0$$；因而矩阵A的F范数平方\(衡量矩阵大小\)  
$$||A||_F^2 = ||\sigma_1 \vec p_1 \vec q_1^T + \sigma_2 \vec p_2 \vec q_2^T + ... + \sigma_k \vec p_k \vec q_k^T||_F^2 = \sigma_1^2||\vec p_1 \vec q_1^T||_F^2 + \sigma_2^2||\vec p_2 \vec q_2^T||_F^2+...+\sigma_k^2||\vec p_k \vec q_k^T||_F^2 = \sigma_1^2 + \sigma_2^2 + ... + \sigma_k^2 = \sum_{i=1}^k\sigma_i^2$$

### **PCA与SVD相似之处：**

1. PCA主要是针对 $$\frac{1}{m}XX^T$$ ，即X的协方差举证进行特征分解
2. SVD则是针对X进行奇异值分解，算的是 $$XX^T,X^TX$$ 的特征值和特征向量，缺少了系数 $$\frac{1}{m}$$ 。从求解方面来说SVD与PCA是等价的。

#### **不同之处：**

        ****PCA 是寻找 $$\frac{1}{m}XX^T$$ 的主要的特征向量作为基向量，对X进行投影，达到降维的目的，即Y=PX。而SVD则是通过保留比较大的奇异值和奇异向量，用秩逼近的形式来近似原矩阵，从而达到降维的目的，虽然殊途同归，但思想是不一样的。

#### **SVD的优点：**

        ****一般 X 的维度很高， $$A^{T}A$$ 的计算量很大,方阵的特征值分解计算效率不高,SVD除了特征值分解这种求解方式外，还有更高效且更准确的迭代求解法，避免了 $$A^{T}A$$ 的计算。

