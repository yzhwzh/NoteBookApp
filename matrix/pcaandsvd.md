---
description: Principal Component Analysis   &  Singular Value Decomposition
---

# 主成分分析与奇异值分解

### 前言：

        矩阵降维与矩阵分解是两个不同的概念。矩阵降维指就是把数据从高维空间投影到一个低维空间，这个过程可以通过线性或者非线性的映射来完成。目的是挖掘出高维数据中富含原始信息的低维嵌入表示。降维显然是有代价的，它造成了原始信息的损失。所以降维算法的重点和难点在于如何在对原始数据进行数据降维的过程中还能尽可能地保持高维数据的几何结构信息或本征的有区别性的信息，并在此前提下找到高维数据的最优低维表示。对于传统的降维算法来说，它们通常的考虑角度都是找到最大可能地保持某种信息的投影方向或者低维空间。而矩阵分解是将矩阵拆解为数个矩阵的乘积，并没有改变矩阵的维数，但通过矩阵分解往往可以从复杂的数据中提取出相对重要的特征信息，如在特征向量为基的各个方向上的投影，然后通过保留较大的投影，删除较小的投影，来实现数据压缩和降维的目的，例如特征分解：

$$
M=A\begin{bmatrix}{\lambda_1}&{0}&{\cdots}&{0}\\{0}&{\lambda2}&{\cdots}&{0}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{0}&{0}&{\cdots}&{\lambda_n}\end{bmatrix}A^{-1}=A\begin{bmatrix}{\lambda_1}&{0}&{\cdots}&{0}\\{0}&{0}&{\cdots}&{0}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{0}&{0}&{\cdots}&{0}\end{bmatrix}A^{-1}+A\begin{bmatrix}{0}&{0}&{\cdots}&{0}\\{0}&{\lambda2}&{\cdots}&{0}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{0}&{0}&{\cdots}&{0}\end{bmatrix}A^{-1}+...+A\begin{bmatrix}{0}&{0}&{\cdots}&{0}\\{0}&{0}&{\cdots}&{0}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{0}&{0}&{\cdots}&{\lambda_n}\end{bmatrix}A^{-1}
$$

，进而可写成秩逼近的形式：

$$
M=\lambda_1[V_1,0,...,0]A^{-1}+\lambda_2[0,V_2,...,0]A^{-1}+...+\lambda_n[0,0,...,V_n]A^{-1} = \lambda_1V_1\mu_1^T + \lambda_2V_2\mu_2^T+...+\lambda_nV_n\mu_n^T
$$

其中 $$V_i$$ 是特征向量，列向量； $$\mu_i^T$$ 为逆矩阵对应的行向量，若是实对称阵 $$\mu_i^T = V_i^T$$ 。故可通过删除小特征值对应方向的数据，只保留K个大特征值方向对应的数据来进行压缩。特征值越大，说明矩阵在对应的特征向量上的方差越大，功率越大，信息量越多，同时也可将矩阵看做是由n\*K，K\*K，K\*n三个小矩阵来近似描述，从而也达到了降维的目的。因而矩阵分解是矩阵降维的一种重要方法。比较常用的矩阵分解方法有EVD和SVD，降维思想则有PCA, LDA, NMF, MDS等，下面介绍一下相关的学习笔记。

### PCA \(Principal Component Analysis\):

        PCA即主成分分析，即设法将原来n个有一定相关的指标，重新组合成一组新的线性无关的综合指标来代替原来指标。也就是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。

> **理解矩阵乘法**
>
> 内积与投影  
>  $$A*B=|A||B|cos\theta$$  含义为 A与B的内积等于A在B上的投影长度乘以B 的模，若设向量B的模为1，则A与B的内积值等于A向B所在直线投影的矢量长度，即 $$A*B=|A|cos\theta$$ 
>
> 基向量  
> 要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了。 例如向量 $$(x,y)$$ 实际上表示线性组合： $$\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix} = x(1,0)^T+y(0,1)^T$$ 
>
> 基变换的矩阵表示  
> 如果我们有M个N维向量，想将其变换为由R个N维向量表示的新空间中，那么首先将R个基**按行**组成矩阵A，然后将向量按列组成矩阵B，那么两矩阵的乘积AB就是变换结果，其中AB的第m列为A中第m列变换后的结果。
>
> **两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。**

        因而PCA的核心，既是要寻找一组基组成的矩阵左乘原矩阵，使得n维特征映射到k维上。那么我们应该如何选择K个基才能最大程度保留原有的信息？

        一种直观的看法是：希望投影后的投影值尽可能分散。而这种分散程度，可以用数学上的方差来表述，即投影后的数值的方差 $$Var(a) = \frac{1}{m}\sum_{i=1}^m(a_i-u)^2$$ ; 如对数据已经均值化了，则 $$Var(a) = \frac{1}{m}\sum_{i=1}^ma_i^2$$ 。方差越大，表明保留的信息越多。

        在高纬度中，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此应该有其他约束条件。从直观上说，让投影尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。协方差表示其相关性，由于已经让每个变量均值为0，则： $$Cov(a,b)=\frac{1}{m}\sum_{i=1}^ma_ib_i$$ ，为了让协方差为0，选择第二个基时只能在与第一个基正交的方向上选择。以此类推，第三个基与第一和第二存在的平面正交。将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各变量两两间协方差为0，而变量的方差则尽可能大（在正交的约束下，取最大的K个方差）。

**PCA 本质**：  
        设原始数据矩阵X，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据（因为X每行的的均值为0，左乘矩阵相当于对X每行乘以一个系数，故变换后，每行均值仍为0）。此时，Y的协方差矩阵为 $$D=\frac{1}{m}YY^T$$ ，根据PCA选择基向量的约束条件，则要求对角线上的方差大，而不同基向量的协方差为0，则D为对角矩阵；进行代换 $$\frac{1}{m}YY^T = \frac{1}{m}PX(PX)^T = \frac{1}{m}PXX^TP^T$$ ,因而可以看出寻找的基向量矩阵，即是为能够让原始矩阵X的协方差阵对角化的矩阵，换句话说，优化目标变成了寻找一个矩阵P，满足 $$PCP^T$$ 是一个对角阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件,\(其中P为C的特征向量组成矩阵的逆矩阵，因为C为对称阵，故P即特征矩阵的转置，前K行即为前K个特征向量\)。

**SVD \(**Singular Value Decomposition\)：

        SVD即奇异值分解，是特征分解在任意矩阵上的推广。因为特征分解，是有约束条件的，矩阵为方阵，并需存在可逆矩阵$$A$$，使得 $$A^{-1}MA$$ 为对角阵，同时还不能保证特征值一定是实数。因而在此基础上对 $$MM^T$$ 进行分解，因为 $$MM^T$$ 是实对称阵，必有一个正交矩阵使之对角化，不同特征值对应的特征向量一定正交，且特征值都是实数，特征向量都是实向量，因而能在任意矩阵上进行应用。

        推导过程：对于任意矩阵, $$A^TA$$ 为$$n$$阶对称矩阵，可以将之作特征分解: $$A^TA=VDV^T$$ ;这个时候我们得到一组正交基 $$\{v_1,v_2,...,v_n\}$$ ，其特征值为 $$\lambda_i$$ ，即 $$A^TAv_i=\lambda_i v_i$$ ; 此时可以推导出 $$\{Av_1,Av_2,...,Av_n\}$$ 也是一组正交基，即 

$$
(Av_i)^T(Av_j)=v_i^TA^TAv_j=v_i^T(\lambda_jv_j)=\lambda_jv_i^Tv_j=0
$$

，只是没有标准化，进一步标准化，令

$$
u_i=\frac{Av_i}{|Av_i|}; |Av_i| = \sqrt{(Av_i)^T(Av_i)} = \sqrt{\lambda_i}
$$

  故 $$Av_i=\sqrt{\lambda_i}u_i=\sigma_i u_i$$ ，将向量组 $${u_1,u_2,... ,u_r}$$ 扩充为 $$R^m$$ 中的标准正交基 $${u_1,u_2,...,u_r,...,u_m}$$ 。则：

$$
AV=A\begin{bmatrix} v_1 & v_2 & ... & v_n \end{bmatrix}= \begin{bmatrix} v_1 & v_2 & ... & v_r & 0 & ... & 0 \end{bmatrix} = \begin{bmatrix} \sigma_1u_1 & \sigma_2 u_2 & ... & \sigma_r u_r & 0 &...&0 \end{bmatrix}= U\Sigma
$$

进而推出 $$A=U\Sigma V^T$$ 这就表明任意的矩阵$$A$$是可以分解成三个矩阵，$$V$$表明了原始域的标准正交基，$$U$$表示经过$$A$$变换后的标准正交基, $$\Sigma$$ 表示了$$V$$中的向量与$$U$$中相对应向量之间的关系。

        在大部分介绍中，一般会这样求解，若 $$AA^T=P\Lambda_1P^T$$ , $$A^TA=Q\Lambda_2Q^T$$ ,则矩阵A的奇异值分解为 $$A=P\Sigma Q^T$$ 。其中$$P$$为左奇异向量矩阵，$$Q$$为右奇异向量矩阵，奇异值则是 $$\sqrt{\lambda_i}$$ （ $$A^TA$$ 的特征值一定非负）; $$AA^T$$ 与 $$A^TA$$ 的对角阵虽然大小不一样，但特征值却是一样的，只是高纬度的可能有重根。这样求解大部分情况是合适的，但当$$A$$ 为对称阵，若有负特征值，会产生误解，因为 $$AA^T=A^TA=A^2$$ ,其特征向量与$$A$$相同，但又要保证奇异值为负，所以会产生一点bug。而通过 $$u_i=\frac{Av_i}{|Av_i|}$$ ，则将负号转移到左奇异矩阵中了。

        同样的，通过SVD，矩阵也能写成秩逼近的形式， $$A = P\Sigma Q^T = \sum_{i=1}^k \sigma_i \vec p_i \vec q_i^T$$ 其中，任意一个 $$\sigma_i \vec p_i \vec q_i^T$$ 是原矩阵的一个秩逼近，所带来的误差是剩余奇异值的平方和。进而可以通过保留前k个秩逼近，来对矩阵进行降维和压缩。如给一个很大的矩阵 $$A_{m*n}$$ 需要储存 $$m*n$$ 个元素，但如果用k个秩和逼近,则只需存储$$k(m+n+1)$$个元素，即$$k$$个奇异值，$$km$$个左奇异向量元素,$$kn$$个右奇异向量元素。

**PCA与SVD相似之处：**

1. PCA主要是针对 $$\frac{1}{m}XX^T$$ ，即X的协方差举证进行特征分解
2. SVD则是针对X进行奇异值分解，算的是 $$XX^T,X^TX$$ 的特征值和特征向量，缺少了系数 $$\frac{1}{m}$$ 。从求解方面来说SVD与PCA是等价的。

**不同之处：**

        ****PCA 是寻找 $$\frac{1}{m}XX^T$$ 的主要的特征向量作为基向量，对X进行投影，达到降维的目的，即Y=PX。而SVD则是通过保留比较大的奇异值和奇异向量，用秩逼近的形式来近似原矩阵，从而达到降维的目的，虽然殊途同归，但思想是不一样的。

**SVD的优点：**

        ****一般 X 的维度很高， $$A^{T}A$$ 的计算量很大,方阵的特征值分解计算效率不高,SVD除了特征值分解这种求解方式外，还有更高效且更准确的迭代求解法，避免了 $$A^{T}A$$ 的计算。

