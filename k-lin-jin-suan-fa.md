# K邻近算法

**K 领近算法：给定一个训练集，对新的输入实例，在新的数据集中找到与该实例最领近的k个实例，这k个实例的多数属于某个类，就把该实例划分为这个类**

算法 1: 输入：训练数据集 $$T=\left{ (x_1,y_1),(x_2,y_2),...,(x_n,y_n) \right}$$ ，其中$x\_i\in \chi \subseteq R^n $为实例的特征向量，$y\_i\in Y = \left{c\_1,c\_2,c\_3...c\_k\right}$ 为实例的类别，$i=1,2,...,N $;实例特征向量x; 输出：实例x所属的类

1\) 根据给定的距离度量，在训练集T中找出与x最领近的k个点，涵盖这k个点的x的邻域记作$N_k\(x\)$; 2\) 在$N\_k\(x\)$中根据分类决策规则（如多数表决）决定x的类别y: $y=argmax_{cj} \sum\_{x\in N\_k\(x\)} I\(y\_i=c\_j\),i=1,2,...,N;j=1,2,...,K$ ;I为指示函数，即当$y\_i=c\_i$时I为1,否则I为0.

2、距离度量  
设特征空间$\chi$ 是n维实数向量空间$R^n,x_i,x\_j \in \chi, x\_i=\(x\_i^{\(1\)},x\_i^{\(2\)},...,x\_i^{\(n\)}\)^T, x\_j=\(x\_j^{\(1\)},x\_j^{\(2\)},...,x\_j^{\(n\)}\)^T, x\_i,x\_j$的Lp的距离为：$$L\_p\(x\_i,x\_j\) = \(\sum_{l=1}^n\|x\_i^{\(l\)}-x\_j^{\(l\)}\|^p\)^{1/p}$$ 当p=2时，是欧式距离；当P=1时，是曼哈顿距离；当p=\infty，它是各个坐标距离的最大值。一般使用的是欧式距离。

3、k值的选择 如果选择较小的K值，用较小的领域中的训练实例进行预测，学习的近似误差会减小，估计误差增大，预测结果会对近邻的实例点非常敏感，如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k值的减小就意味着整体模型变得复杂，容易发生过拟合。 如果选择较大的K值，误差会相反，但较远的训练实例也会对预测起作用，使预测发生错误。K值的增大就意味着整体的模型变得简单。 在应用中，K值一般选择一个比较小的数值，通常采用交叉验证法来选取最优的K值。

4、分类决策规则： 分类决策规则往往是多数表决，既由输入实例的k个邻近的训练实例中的多数类决定输入的类 如果分类的损失函数为0-1；分类函数为$ f:R^n-&gt;{c_1,c\_2,...,c\_k } $,那么误分类概率是 $P\(Y\neq f\(X\)\)=1-P\(Y = f\(X\)\)$;因而给定的实例$x \in \chi$,其最近邻的k个训练实例点构成集合$N\_k\(x\)$,如果涵盖$N\_k\(x\)$的区域类别是$c\_j$,那么误分类率是 $$1/k \sum_{x_i\in N\_k\(x\)} I\(y\_i \neq c\_j\) = 1-1/k \sum_{x_i\in N\_k\(x\)} I\(y\_i = c\_j\)$$; 要使误分类率最小，就要使$1/k \sum_{x\_i\in N\_k\(x\)} I\(y\_i = c\_j\)$ 最大，所以多数表决规则等价与经验风险最小化。

5、背景知识，树相关介绍 树的定义：一棵树是一些节点的集合，这个集合可以是空集；若非空，则一棵树由称做根的节点r以及0个或多个非空的树T1,T2,...,Tk组成，这些子树中每一棵的根都被来自根r的一条有向的边所连接。 树的实现：将每个节点的所有儿子都放到树节点的链表中。

常用的几个树结构： 1、二叉树: 每个节点都不能多于两个儿子， 平均深度 $O\(\sqrt N\)$, 若是二叉查找树，其深度平均值是$O\(log N\)$。最坏的情况，输得深度能达到N-1 用途： 表达式树，树叶为操作数，其他节点为操作符；遍历方法的不同，会导致最终树的赋值也不同，一般的方法为中序遍历。 查找树ADT-二叉查找树： 对于树中每个节点X，它的左子树中所有关键字值小于X的关键字值，右子树大于X的关键字值。 AVL树，二叉查找树加上了平衡条件，每个节点的左子树与右子树的高度最多差1 伸展树：当一个节点被访问后，他就要经过一系列AVL树的旋转被放到根上，保证对树的操作最多花费$O（MlogN）$

```text
2、B-树 ：树的根或者是一片树叶，或者其儿子数在2和M之间；除根外，所有非树叶节点的儿子数在[M/2]和M之间；所有树叶都在相同的深度上；M为深度。
```

6、K邻近，kd树，是二叉树，表示对k维空间的一个划分。构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。 输入：k维空间数据集$T={ x\_1,x\_2,...,x\_N }$；其中$x\_i = {x\_i^{\(1\)},x\_i^{\(2\)},...,x\_i^{\(k\)}},i=1,2,3...N$ 输出 kd树 1\) 构造根节点： 选择以$x^{\(1\)}$ 为坐标轴，以T中所有实例的$x^{\(1\)}$的中位数为切分点，将根节点对应的超矩形区域分为两个子区域.切分由通过切分点并与坐标轴$x^{\(1\)}$垂直的超平面实现。由根节点生成深度为1的左右子结点；左子结点对应坐标$x^{\(1\)}$小于切分点的区域，右子节点对应于大于切分点的区域。 2\) 重复：对深度为j的结点，选择$x^{\(l\)}$为切分的坐标轴，$l=j\(mod k\)+1$,以该结点区域中所有实例的$x^{\(l\)}$坐标的中位数为切分点，将该结点对应的超矩形区域分为两个子区域，切分由通过切分点并与坐标轴$x^{\(l\)}$垂直的超平面实现。 3\) 直到两个子区域没有实例存在时停止。从而形成kd树的区域划分。

7、用kd树的最近邻搜索 输入：已构造的kd树；目标点x 输出：x的最近邻

```text
1) 在kd树中找出包含目标点x的叶节点：从根结点出发，递归地向下访问kd树。若目标点x当前维的坐标小于切分点的坐标，则移动到左子节点，都则移动到右子节点。直到子节点为叶节点为止。
2) 以此叶节点为“当前最近点”。
3) 递归地向上回退，在每个结点进行以下操作：
    a) 如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”
    b) 当前最近点一定存在于该结点一个子结点对应的区域。检查该子节点的父节点的另一子节点对应的区域是否有更近的点
4) 当回退到根节点时，搜索结果，最后的“当前最近点”即为x的最近邻点。
```

