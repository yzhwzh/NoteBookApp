# 朴素贝叶斯

相关基础知识:

贝叶斯定理$$P(H|D) = \frac{P(H)P(D|H)}{P(D)}$$ \#$$P(H) $$称为先验概率，$$P(H|D)$$为后验概率，即需计算的概率；$$P（D|H）$$称为似然度，$$P（D）$$称为标准化常量

1、模型 朴素贝叶斯法通过训练数据集学习联合概率分布$$P(X,Y)$$. 具体地，先验概率分布 $$P(Y=c_k),k=1,2,...,K$$_; 条件概率分布_ $$P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)},...,X^{(N)}=x^{(N)}|Y=c_k),k=1,2,...,k$$假设$$x^{(j)}$$_可取值有_$$S_j$$_个，_$$j-1,2,...,n,$$ __$$Y$$_可取值有_$$K$$_个，那么参数个数为_$$K \prod_{j=1}^n S_j$$\_\_

**朴素贝叶斯对条件概率分布做了条件独立性的假设，由于这是一个较强的假设，朴素贝叶斯法由此得。条件独立性假设** $$P(X=x | Y=ck) = P(X^{(1)}=x^{(1)},...,X^{(N)}=x^{(N)}|Y=c_k) = \prod_{j=1}^n P（X^{(j)}=x^{(j)}|Y=c_k）$$ ****

 **条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率** 

用于分类时，对给定的输入$$x$$,通过学习到的模型计算后验概率分布$$P(Y=c_k|X=x)$$,将后验概率最大的类作为$$x$$的类输出。后验概率计算根据贝叶斯定理进行$$P(Y=c_k | X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_k P(X=x|Y=c_k)P(Y=c_k)}$$ **不是很明白此处的公式，强行理解为每一类X的概率相加，可能就为**$$P(X=x)$$ ****

故朴素贝叶斯公式：$$ y=f(x)=argmax{c_k} \frac{P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=ck)}{\sum_k P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=ck)}$$ _因为分母对所有_$$c_k$$_都是相同的，所以_$$y=argmax{ck}P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)$$**可以理解为分母是个常量？**

2、损失函数 属于分类问题，选择$$0-1$$损失函数 $$L(Y,f(X))= \left\{\begin{array}{rl }+1,&Y \neq f(x)\\-1,&Y=f(x)\end{array} \right.$$ 这时期望风险函数为：$$R_{exp}(f) = E[L(Y,f(x))] = E_x\sum_{k=1}^K[L(c_k,f(X))]P[c_k|X]$$ 为了使期望风险最小化，只需要对$$X=x$$逐个极小化，由此得到 $$f(x) = argmin_{y\in Y} \sum_{k=1}^{K} L(c_k,f(X))P(c_k|X=x) = argmin_{y\in Y} \sum_{k=1}^{K} P(y \neq c_k | X=x) =argmin_{y\in Y} (1-P(y=c_k|X=x)) = argmax_{y\in Y} P(y=c_k|X=x)$$ 即朴素贝叶斯法所采用的原理。

3、算法 极大似然估计 先验概率$$P(Y=c_k)$$ _的极大似然估计_ $$P(Y=c_k) = \frac{\sum_{i=1}^N I(yi = c_k)}{N},k=1,2,...,K $$_设第_$$j$$_个特征_$$x^{(j)}$$_可能取值的集合为_$${a_{j1},a_{j2},...,a_{jn}}$$_，条件概率_$$P(X^{(j)}=a_{jl}|Y=c_k)$$_的极大似然估计是_$$P(X^{(j)}=a{jl}|Y=ck) = \frac{\sum{i=1}^N I(xi^{(j)}=a{jl},yi=c_k)}{\sum{i=1}^N I(yi=c_k)}, j=1,2,...,n; l=1,2,...,S_j; k=1,2,...,K; x_i^{(j)} $$_是第_$$i$$_个样本的第_$$j$$_个特征；_ $$a_{ij}$$是第$$j$$个特征可能取的第$$l$$个值，$$I$$为指示函数。

4、学习与分类算法

输入：训练数据$$T={(x1,y_1),(x_2,y_2),...,(x_N,y_N)},其中x_i = (x_i^1,x_i^2,...,x_i^n)^T,x_i^j$$_是第_$$i$$_个样本的第_$$j$$_个特征，_$$x_i^j \in {a_{j1},a_{j2},...,a_{jn}}, a_{jl} $$是第$$j$$个特征可能的第$$l$$个值，$$j=1,2,...,n, l=1,2,...,S_j, y_i \in {c_1,c_2,...,c_k}$$;实例 $$x$$； 输出： 实例$$x$$的分类

步骤 1\) 计算先验概率和条件概率 $$P(Y=c_k) = \frac{\sum_{i=1}^N I(y_i = c_k)}{N},k=1,2,...,K$$ $$P(X^{(j)}=a_{jl}|Y=c_k) = \frac{\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^N I(y_i=c_k)}$$

2\) 对于给定的实例 $$x=(x^1,x^2,...,x^n)^T 计算P(Y=ck)\prod{j=1}^n P（X^{(j)}=x^{(j)}|Y=c_k)$$

3\) 确定实例$$x$$的类$$y=argmax_{c_k}P(Y=c_k)\prod_{j=1}^n P（X^{(j)}=x^{(j)}|Y=c_k)$$

5、贝叶斯估计 即在朴素贝叶斯算法上加入拉普拉斯平滑 $$P(X^{(j)}=a_{jl}|Y=c_k) = \frac{\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^N I(y_i=c_k)+S_j\lambda}$$ 公式中$$\lambda>=0$$,等价于在随机变量各个取值的频数上赋予一个正数 $$\lambda>0$$。 当$$\lambda=0$$时，就是极大似然估计，常取$$\lambda = 1$$,这时称为拉普拉斯平滑

先验概率的贝叶斯估计 $$P(Y=c_k) = \frac{\sum_{i=1}^N I(y_i = c_k)+\lambda}{N+K\lambda}$$

