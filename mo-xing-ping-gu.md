# 模型评估

### 泛化能力

1. 为了评估机器学习算法的能力，必须给定其性能的衡量指标。
2. 有些情况下，很难决定衡量指标是什么：
   * 如：翻译任务中，应该衡量整个翻译结果的准确率，还是衡量每个单词翻译的准确率？
   * 如：密度估计任务中，很多模型都是隐式地表示概率分布。此时计算样本空间某个点的真实概率是不可行的，因此也就无法判断该点的概率估计的准确率。
3. 通常利用最小化训练误差来训练模型，但是真正关心的是测试误差。**因此通过测试误差来评估模型的泛化能力。**
   * 训练误差是模型在训练集的平均损失，其大小虽然有意义，但是本质上不重要。
   * 测试误差是模型在测试集上的平均损失，反应了模型对未知测试数据集的预测能力。
4. **模型对未知数据的预测能力称作模型的泛化能力，它是模型最重要的性质。**

   泛化误差可以反映模型的泛化能力：泛化误差越小，该模型越有效。

5. 假设训练集和测试集共同的、潜在的样本分布称作数据生成分布，记作 $$p_{data}(\mathbf{\vec x},y)$$。则泛化误差定义为模型的期望风险，即： $$R_{exp}(f)=\mathbb E[L(y, f(\mathbf{\vec x}))]=\int_{\mathcal{X \times Y}}L(y, f(\mathbf{\vec x}))p_{data}(\mathbf {\vec x},y)d \mathbf {\vec x}\; dy$$ 
   * 通常泛化误差是不可知的，因为无法获取联合概率分布 $$p_{data}(\mathbf{\vec x},y)$$ 以及无限的采样点。
   * 现实中通常利用测试误差评估模型的泛化能力。由于测试数据集是有限的，因此这种评估结果不完全准确。
6. 统计理论表明：如果训练集和测试集中的样本都是独立同分布产生的，则有 **模型的训练误差的期望等于模型的测试误差的期望** 。
7. 机器学习的“没有免费的午餐定理”表明：在所有可能的数据生成分布上，没有一个机器学习算法总是比其他的要好。
   * 该结论仅在考虑所有可能的数据分布时才成立。
   * **现实中特定任务的数据分布往往满足某类假设，从而可以设计在这类分布上效果更好的学习算法。**
   * 这意味着机器学习并不需要寻找一个通用的学习算法，而是寻找一个在关心的数据分布上效果最好的算法。
8. 正则化是对学习算法做的一个修改，这种修改趋向于降低泛化误差（而不是降低训练误差）。
   * 正则化是机器学习领域的中心问题之一。
   * **没有免费的午餐定理说明了没有最优的学习算法，因此也没有最优的正则化形式。**

### 过拟合、欠拟合

1. 当使用机器学习算法时，决定机器学习算法效果的两个因素：降低训练误差、缩小训练误差和测试误差的差距。

   这两个因素对应着机器学习中的两个主要挑战：欠拟合和过拟合。

2. 过拟合`overfitting`：**选择的模型包含的参数过多**，以至于该模型对于已知数据预测得很好，但是对于未知数据预测的很差，使得训练误差和测试误差之间的差距太大。
   * 过拟合的原因是：将训练样本本身的一些特点当作了所有潜在样本都具有的一般性质，这会造成泛化能力下降。
   * 过拟合无法避免，只能缓解。因为机器学习的问题通常是`NP`难甚至更难的，而有效的学习算法必然是在多项式时间内运行完成。如果可以避免过拟合，这就意味着构造性的证明了`P=NP` 。
3. 欠拟合`underfitting`：**选择的模型包含的参数太少，以至于该模型对已知数据都预测的很差**，使得训练误差较大。

   欠拟合的原因一般是学习能力低下造成的。

4. **通过调整模型的容量`capacity`可以缓解欠拟合和过拟合。**

#### 模型容量

1. **模型的容量是指其拟合各种函数的能力。**
   * **容量低的模型容易发生欠拟合，模型拟合能力太弱。**
   * **容量高的模型容易发生过拟合，模型拟合能力太强。**
2. 通过选择不同的假设空间可以改变模型的容量。

   模型的假设空间指的是：代表模型的函数集合。这也称作模型的表示容量`representational capacity`。

   由于额外的限制因素（比如优化算法的不完善），模型的有效容量`effective capacity`一般会小于模型的表示容量。

3. 通常在模型的假设空间中出最佳的函数是非常困难的优化问题，实际应用中只是挑选一个使得训练误差足够低的函数即可。
4. 统计学习理论提供了**量化模型容量**的方法，其中最出名的是**`VC`维理论**：**训练误差与泛化误差之间差异的上界随着模型容量增长而增长，随着训练样本增多而下降** 。
5. 虽然`VC` 维理论对于机器学习算法有很好的指导作用，但是它在深度学习很难应用。原因有二：
   * 边界太宽泛。
   * 难以确定深度学习的容量。由于深度学习模型的有效容量受限于优化算法，因此确定深度学习模型的容量特别困难。
6. 通常泛化误差是关于模型容量的 `U`形函数。随着模型容量增大：
   * 训练误差会下降直到逼近其最小值。
   * 泛化误差先减小后增大。
   * 泛化误差与训练误差的差值会增大。

![](.gitbook/assets/image%20%2815%29.png)

#### 缓解过拟合

1. 缓解过拟合的策略：

   * 正则化。
   * 数据集增强：通过人工规则产生虚假数据来创造更多的训练数据。
   * 噪声注入：包括输入噪声注入、输出噪声注入、权重噪声注入。将噪声分别注入到输入/输出/权重参数中。
   * 早停：当验证集上的误差没有进一步改善时，算法提前终止。

   > 具体内容参考深度学习《正则化》章节。

2. 正则化 ：基于结构化风险最小化（`SRM`）策略的实现，其中 $$J(f)$$ 为正则化项。

   在不同的问题中，正则化项可以有不同的形式：

   * 回归问题中，损失函数是平方损失，正则化项是参数向量的 $$L_2$$ 范数。
   * 贝叶斯估计中，正则化项对应于模型的先验概率 $$\log \frac{1}{g(\theta)}$$ 。

#### 缓解欠拟合

缓解欠拟合的策略：**选择一个模型容量更高的模型**

### 偏差方差分解

#### 点估计

1. 点估计：对参数 $$\theta$$ 的一个预测，记作 $$\hat \theta$$ ****。

   假设 $$\{x_1,x_2,\cdots,x_m\}$$ 为独立同分布的数据点，该分布由参数 $$\theta$$ 决定。则参数 $$\theta$$ 的点估计为某个函数： $$\hat\theta_m =g(x_1,x_2,\cdots,x_m)$$  

   注意：点估计的定义并不要求  返回一个接近真实值 $$\theta$$ 。

2. 根据频率学派的观点：
   * 真实参值 $$\theta$$ 是固定的，但是未知的。
   *  $$\hat\theta_m$$ 是数据点的函数。
   * 由于数据是随机采样的，因此 $$\hat\theta_m$$ 是个随机变量。

#### 偏差

1. 偏差定义为： $$bias(\hat\theta_m)=\mathbb E(\hat\theta_m)-\theta$$ ，期望作用在所有数据上。
   * 如果 $$bias(\hat\theta_m)=0$$  ，则称估计量 $$\hat\theta_m$$  是无偏的。
   * 如果 $$\lim_{m\rightarrow \infty}bias(\hat\theta_m)=0$$ ，则称估计量 $$\hat\theta_m$$ 是渐近无偏的。
2. 无偏估计并不一定是最好的估计。
3. 偏差的例子：
   * 一组服从均值为 $$\theta$$  的伯努利分布的独立同分布样本 $$\{x_1,x_2,\cdots,x_m\}$$  ： $$\hat\theta_m=\frac 1m\sum_{i=1}^{m}x_i$$  为 $$\theta$$ 的无偏估计。
   * 一组服从均值为 $$\mu$$ ，方差为 $$\sigma^{2}$$  的高斯分布的独立同分布样本 ：
     *  $$\hat\mu_m=\frac 1m\sum_{i=1}^{m}x_i$$ 为 $$\mu$$ 的无偏估计。
     *  $$\hat\sigma^{2}_m=\frac 1m\sum_{i=1}^{m}(x_i-\hat\mu_m)^{2}$$ 为 $$\sigma^{2}$$ 的有偏估计。因为 $$\mathbb E[\hat\sigma^{2}_m]=\frac{m-1}{m}\sigma^{2}$$ 
     *  $$\tilde\sigma^{2}_m=\frac {1}{m-1}\sum_{i=1}^{m}(x_i-\hat\mu_m)^{2}$$ 为 $$\sigma^{2}$$ 的无偏估计。

#### 一致性

1. 通常希望当数据集的大小 $$m$$ 增加时，点估计会收敛到对应参数的真实值。即： $$\text{plim}_{m\rightarrow \infty}\hat\theta_m=\theta$$ 表示依概率收敛。即对于任意的 $$\epsilon \gt 0$$ ，当 $$m\rightarrow \infty$$ 时，有： $$P(|\hat\theta_m -\theta|)\gt \epsilon \rightarrow 0$$ 
2. 上述条件也称做一致性。它保证了估计偏差会随着样本数量的增加而减少。
3. 渐近无偏不一定意味着一致性。

       如：在正态分布产生的数据集中，可以用 $$\hat\mu_m=x_1$$  作为 $$\mu$$  的一个估计。

   1. 它是无偏的，因为 ，所以不论观测到多少个数据点，该估计都是无偏的
   2. 但它不是一致的，因为他不满足

