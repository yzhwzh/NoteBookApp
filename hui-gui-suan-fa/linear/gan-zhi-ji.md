---
description: Perceptron
---

# 感知机

### 介绍：

        感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。

#### 相关基础知识储备：

* 向量的积，也称点积和内积

在数学中，数量积（dot product; scalar product，也称为点积）是接受在实数R上的两个向量并返回一个实数值标量的二元运算。它是欧几里得空间的标准内积。

设二维空间内有两个向量 $$\vec{a} = (x_1,y_1), \vec{b} = (x_2,y_2)$$ 定义它们的数量积（又叫内积、点积）为以下实数： $$\vec{a} \cdot \vec{b} = x_1x_2+y_1y_2$$ ，更一般地，n维向量的内积定义如下 $$\vec{a} \cdot \vec{b} = \Sigma_{i=1}^n a_ib_i$$ ; 几何定义： $$\vec{a} \cdot \vec{b} = |\vec{a}| \cdot |\vec{b}| \cdot cos\theta$$ 。即 $$\Sigma_{i=1}^n a_ib_i = |\vec{a}| \cdot |\vec{b}| \cdot cos\theta$$ ； 这可以通过余弦公式得证明 $$c^2 = a^2+b^2-2|a||b|cos\theta$$ 

以二维向量为例： $$\vec{c} = \vec{b}- \vec{a} = (x_2-x_1,y_2-y_1)$$ ; 故 $$(x_2-x_1)^2+(y_2-y_1)^2 = x_1^2+y_1^2+x_2^2+y_2^2-2|\vec{a}||\vec{b}|cos\theta$$ .

最终得到 $$|\vec{a}||\vec{b}|cos\theta = x_1x_2+y_1y_2 = \vec{a}\cdot\vec{b}$$ .

 **拓展知识：分清向量积（矢积）与数量积（标积）的区别** 

| **名称** | **标积 / 内积 / 数量积 / 点积** | **矢积 / 外积 / 向量积 / 叉积** |
| :--- | :--- | :--- |
| 运算式（a，b和c粗体字，表示向量） | $$a·b=|a||b|·cosθ$$ **** | a×b=c,其中 $$|c|=|a||b|·sinθ$$ ,c的方向遵守右手定则 |
| 几何意义 | 向量a在向量b方向上的投影与向量b的模的乘积 | c是垂直a、b所在平面，且以 $$|b|·sinθ$$ 为高、 $$|a|$$ 为底的平行四边形的面积 |
| 运算结果的区别 | 标量（常用于物理）/数量（常用于数学） | 矢量（常用于物理）/向量（常用于数学） |

向量积 $$|c|=|a×b|=|a| |b|sin<a,b>$$ ,

即c的长度在数值上等于以a，b，夹角为θ组成的平行四边形的面积。

设 $$\vec{a} = (x_1,y_1,z_1) , \vec{b}= (x_2,y_2,z_2)$$ ，i，j，k分别是X，Y，Z轴方向的单位向量，则外积

$$\vec{a}X\vec{b} = det\left| \begin{array}{ccc}i&j&k\\x_1&y_1&z_2\\x_2&y_2&z_2 \end{array} \right|$$ ；

同时 $$\vec{a} X \vec{b}  =  (x_1,y_1,z_1) X  (x_2,y_2,z_2)  = (y_1z_2-z_1y_2,z_1x_2-x_1z_2,x_1y_2-y_1x_2)$$ 

* 超平面：给定向量空间 $$R_n$$ 中的一个点 P 和一个非零向量 $$\vec{n}$$ ,满足 $$\vec{n}*(i-p)=0$$ 则称点集 i 为通过点 $$P$$ 的超平面，向量 $$\vec{n}$$ 为通过超平面的法向量。按照这个定义，虽然当维度大于3才可以成为“超”平面，但是你仍然可以认为，一条直线是 $$R_2$$ 空间内的超平面，一个平面是 $$R_3$$ 空间内的超平面；$$R_n$$ 空间的超平面是 $$R_n $$ 空间内的一个 $$n - 1$$ 维的仿射子空间。

> 理解推导一下,直线是二维空间中的超平面，直线的表示方式为 $$ax+by+c=0 => y=-(ax+c)/b$$ ，写成向量形式也是 $$W^TX+b = 0$$

> n 维空间中的超平面由下面的方程确定: $$W^TX+b = 0$$ 其中，w 和 x 都是 n 维列向量，x 为平面上的点，w 为平面上的法向量，决定了超平面的方向。

**点到平面的距离：** $$d=|Ax_0+By_0+Cz_0+b|/\sqrt{A^2+B^2+C^2} $$ ****。

#### **感知机模型**

**决策函数**：

$$
f\left( x \right) = sign(w\cdot x+b) = \left\{\begin{array}{rl }+1,&w\cdot x_i+b>=0\\-1,&w\cdot x_i+b<0\end{array} \right.
$$

，其中w和b为感知机模型参数， $$w\in R^n$$ 为权值向量， $$b\in R$$ 叫做偏置； 感知机是一种线性分类模型，属于判别模型，其几何意义为特征空间 $$R^n$$ 中的一个超平面S $$(w^T\cdot x+b =0)$$ 将特征空间划分为两个部分，位于两部分的点分别分为正负两类。如下图所示：

**损失函数**：

损失函数的自然选择是误分类点的总数，但这样的损失函数不是参数w,b的连续可导的函数，不易优化。损失函数的另一个选择是误分类点到超平面S的总距离，这是感知机所采用的。输入空间 $$R^n$$ 中的任意一点到平面S的距离： $$\frac{1}{||w||}|w\cdot x_0+b|$$ 这里，\|\|w\|\|是w的 $$L_2$$ 范数。

其次对于误分类的数据 $$(x_i,y_i)$$ 来说 $$-y_i(w \cdot x_i+b)>0$$ 成立，因为 $$w\cdot x_i+b>0$$ 误分类点 $$y_i=1$$ 会被判为-1, 反之 $$y_i=-1$$ 亦然。

因而误分类点到超平面的距离 $$-\frac{1}{||w||}y_i(w\cdot x_i+b)$$ ,若所有误分类点集合为M，那么所有误分类点到超平面S总距离为 $$-\frac{1}{||w||}\Sigma_{x_i\in M}y_i(w\cdot x_i+b) $$ 不考虑 $$\frac{1}{||w||}$$ ，就得到感知机的loss function.

给定训练集后损失函数为 $$L(w,b)=-\Sigma_{x_i\in M}y_i(w\cdot x_i+b)$$ ，该函数非负，若没有误分类点，损失函数为0。误分类点越少，误分类点离超平面越近，损失函数值就越小。一个特定的样本点的损失函数，在误分类时，参数w,b是线性函数，因而可导。

#### 学习算法：

感知机学习算法是**误分类驱动的**，具体采用随机梯度下降法 stochastic gradient descent。 **极小化过程中不是一次使M中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降**

损失函数L\(w,b\)的梯度为 $$\nabla_w L(w,b) = -\Sigma_{x_i\in M} y_ix_i ; \nabla_b L(w,b) = -\Sigma_{x_i\in M} y_i$$ 。 随机选取一个误分类点 $$(x_i,y_i)$$ ，对w,b进行更新： $$w\leftarrow w+\eta y_ix_i ; b\leftarrow b+\eta y_i $$ 其中 $$\eta( 0<\eta<=1)$$ 是步长，即学习率，这样通过迭代可以期待损失函数不断减小，直到为0

整理后： 感知机原始形式

输入： 训练数据集 $$T=\left\{ (x_1,y_1),(x_2,y_2),...,(x_n,y_n) \right\}$$ ,其 $$ x_i\in \chi = R^n,y_i \in y = \left\{ -1,+1\right\},i=1,2,3.,4...n: $$ 学习率 $$\eta (0<\eta<=1)$$ ;

输出：w,b; 感知机模型 $$f\left( x \right) = sign(w\cdot x+b)$$ 

1）选取值 $$w_0,b_0$$ 

2\) 在训练集中选取数据 $$(x_i,y_i)$$ 

3\) 如果 $$y_i(w \cdot x_i+b)<=0 ; w\leftarrow w+\eta y_ix_i ; b\leftarrow b+\eta y_i$$ 

4\) to 2\) 直至训练集中没有误分类点

解释： 当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整w,b的值，使分离平面向该误分类点的一侧移动，以减小该误分类点与超平面的距离，直至超平面越过该误分类点使其被正确分类。

感知机学习算法由于采取不同的初值或选取不同的误分类点，解可以不同。

定理：感知机算法 $$f\left( x \right) = sign(w\cdot x+b)$$ 在训练数据集上的误分类次数k满足不等式 $$k<=(\frac{R}{\gamma})^2, R=max_{1<=i<=N}||\hat{x_i}||$$ 即误分类次数k是有上界的，经过有限次搜索可以将训练数据完全分开 （李航P32）

感知机对偶形式：基本思想将w和b表示为实例 $$x_i$$ 和 $$y_i$$ 的线性组合形式，即 $$w\leftarrow w+\eta y_ix_i；b\leftarrow b+\eta y_i$$ 逐步修改w，b，设修改n次，则w，b 关于 $$(x_i,y_i)$$ 的增量分别是 $$\alpha_iy_ix_i$$ 和 $$\alpha_iy_i$$ ; $$\alpha_i=n_i\eta$$ ;若初始值 $$w_0,b_0$$ 均为0，最后学习到的 $$w=\Sigma_{i=1}^N\alpha_ix_iy_i;b=\Sigma_{i=1}^N\alpha_iy_i$$ 

步骤：

输入： 训练数据集 $$T=\left\{ (x_1,y_1),(x_2,y_2),...,(x_n,y_n) \right\}$$ ,其中 $$x_i\in \chi = R^n,y_i \in y = \left\{ -1,+1\right\},i=1,2,3.,4...n:$$ 学习率 $$\eta (0<\eta<=1)$$ ;

输出： $$\alpha ,b$$ ; 感知机模型 $$f\left( x \right) = sign(\Sigma_{i=1}^N\alpha_ix_iy_i\cdot x+b)$$ 其中 $$\alpha = (\alpha_1,\alpha_2,\alpha_3...\alpha_N)^T$$ 

1\) $$\alpha\leftarrow0,b\leftarrow0 $$ 

2\) 训练集中选取数据 $$(x_i,y_i)$$ 

3）如果 $$y_i(\Sigma_{i=1}^N\alpha_ix_iy_i\cdot x+b)<=0; \alpha\leftarrow\alpha+\eta;b\leftarrow b+\eta y_i$$ 

4） to 2）直至没有误分类数据

对偶形式中的训练实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的Gram矩阵， $$G=[x_i\cdot x_j]_{NxN} $$ 

不懂之处 ：

1. 对偶形式与原适形式的区别即随即梯度与批量梯度下降的区别
2. Gram矩阵：

$$
matrix \left( \begin{array}{ccc} x_1^Tx_1 & x_1^Tx_2 & x_1^Tx_3\\ x_2^Tx_1 & x_2^Tx_2 & x_2^Tx_3 \\ x_3^Tx_1 & x_3^Tx_2 & x_3^Tx_3 \end{array} \right)
$$

