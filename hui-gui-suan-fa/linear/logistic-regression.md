---
description: Logistic Regression
---

# 对数几率回归

### 介绍

前章的线性模型，基本假设是Y服从正态分布，广义线性模型对其进行了拓展，其假设标签Y服从某种分布，该分布的参数线性的依赖于样本。

通过联结函数建立响应变量的数学期望值与线性组合的预测变量之间的关系。其特点是不强行改变数据的自然度量，数据可以具有非线性和非恒定方差结构。**是线性模型在研究响应值的非正态分布以及非线性模型简洁直接的线性转化时的一种发展**。

可以理解为通过单调可微函数 $$g(.)$$ ，令 $$\hat \beta_0 + \hat\beta_1X_1+\hat\beta_2X_2+...+\hat\beta_kX_k $$ 逼近y的衍生物，可写为 $$y=g^{-1}(w^Tx+b)$$ ;例如 $$lny = w^Tx+b$$ 即是**对数线性回归**，虽然形式上仍是线性回归，但实质上已是求取输入空间到输出空间的非线性函数映射。

###  对数几率回归，即Logistic Regression

考虑二分类任务，只需将线性回归模型预测的实值转换为0/1值，最理想的是"单位阶跃函数（Heaviside）"，大于0就判为正例，小于零则判为反例，预测值为临界值则可任意判别。因而在线性回归的基础上，引入了阶跃函数：

$$
y = \frac{1}{1+e^{-z}}
$$

该函数为Sigmoid函数，是一种函数曲线形似S的函数，将其作为单调可微函数 $$g(.)$$ 装入线性公式，

$$
y = \frac{1}{1+e^{-(w^Tx+b)}}
$$

变形后得：

$$
ln\frac{y}{1-y} = w^Tx+b
$$

若将 $$y$$ 视为样本 $$x$$ 作为正的可能性，则 $$1-y$$ 是其反例的可能性，两者的比值称为几率（odds），反映了 $$x$$ 作为正例的可能性，对几率取对数则得到对数几率（log odds 亦称logit），即 $$ln\frac{y}{1-y}$$ 

由此可看出，实际上是用线性回归模型的预测结果去逼近真实标记的对数几率，因此，其对应的模型称为对数几率回归（logistic regression或logit regression）。

若将 $$y$$ 视为类后验概率估计 $$p(y=1|x)$$ ，则 $$ln\frac{p(y=1|x)}{p(y=0|x)} = w^Tx+b$$ ，进而：

$$
p(y=1|x) = \frac{1}{1+e^{-(w^Tx+b)}} = \frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}
$$

$$
p(y=0|x)  = \frac{1}{1+e^{w^Tx+b}}
$$

可通过极大似然估计求解，将b和w合并为一个向量，w扩充一个1向量，

$$
h(x)=p(y=1|x)= \frac{1}{1+e^{-w^Tx}}
$$

$$y_i$$ 为类别标签，取值为1或0。给定参数w和样本特征向量x，样本属于每个类的概率可以统一写成如下形式：

$$
p(y|x,w) = (h(x))^y(1-h(x))^{1-y}
$$

最大似然乘积为：

$$
L(w) = \prod_{i=1}^l p(y_i|x_i,w) = \prod_{i=1}^l(h(x_i)^{y_i} (1-h(x_i))^{1-y_i})
$$

$$
ln L(W) = \sum_{i=1}^l(y_ilnh(x_i)+(1-y_i)ln(1-h(x_i)))
$$

求取 $$ln L(W)$$ 最大值，等价于求取 $$-ln L(W)$$ 最小值， 通过梯度下降法，求极值点，



$$
-\frac{\partial ln L(W) }{\partial W} = \sum_{i=1}^l(h(x_i)-y_i)x_i
$$

最后得到系数矩阵的梯度下降法迭代更新公式为：

$$
W_{k+1} = W_k - \alpha \sum_{i=1}^l (h_w(x_i) - y_i) x_i
$$

